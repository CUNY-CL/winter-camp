{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1c9b0e8",
   "metadata": {},
   "source": [
    "# True-casing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb88fb7",
   "metadata": {},
   "source": [
    "[Kyle Gorman](mailto:kgorman@gc.cuny.edu), [M. Elizabeth\n",
    "Garza](mailto:garza.elizabeth9@gmail.com), and [Emily\n",
    "Campbell](mailto:ecampbell4@gradcenter.cuny.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930c7aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7b77ff",
   "metadata": {},
   "source": [
    "## Learning goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2659036",
   "metadata": {},
   "source": [
    "In this exercise, you will learn about a simple but important NLP task\n",
    "called *true-casing* or *case restoration*, which allows one to\n",
    "-   restore missing capitalization in noisy user-generated text as is\n",
    "    often found in text messages (SMS) or posts on social media,\n",
    "-   add capitalization to the output of [machine\n",
    "    translation](https://en.wikipedia.org/wiki/Machine_translation) and\n",
    "    [speech\n",
    "    recognition](https://en.wikipedia.org/wiki/Speech_recognition) to\n",
    "    make them easier for humans to read, or even\n",
    "-   transfer the \"style\" of casing from one collection of documents to\n",
    "    another.\n",
    "As part of this exercise, you will build your own case restoration\n",
    "system using Python and command-line tools. As such you will get\n",
    "practice using Python to read and writing text files, and using the\n",
    "command line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10970fca",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330a3578",
   "metadata": {},
   "source": [
    "You should have a working familiarity with\n",
    "[Python](https://www.python.org/) and be comfortable using data\n",
    "structures like lists and dictionaries, calling functions, opening\n",
    "files, importing built-in modules, and reading technical documentation.\n",
    "\n",
    "You do not need to be familiar with the [conditional random field\n",
    "models](https://en.wikipedia.org/wiki/Conditional_random_field)\n",
    "(Lafferty et al. 2001), one of the technologies we use, but it may be\n",
    "useful to read a bit about this technology before beginning.\n",
    "\n",
    "The exercise is intended to take several days; at the [Graduate\n",
    "Center](https://www.gc.cuny.edu/Page-Elements/Academics-Research-Centers-Initiatives/Doctoral-Programs/Linguistics/Linguistics),\n",
    "master’s students in computational linguistics often complete it as a\n",
    "supplemental exercise over winter break, after a semester of experience\n",
    "learning Python. (Hence the name \"Winter Camp.\")\n",
    "\n",
    "This tutorial assumes you have access to a UNIX-style command line\n",
    "interface:\n",
    "-   On Windows 10 onward, you access a command line using [Windows\n",
    "    Subsystem for\n",
    "    Linux](https://docs.microsoft.com/en-us/windows/wsl/install-win10);\n",
    "    the Ubuntu \"distro\" (distribution) is particularly easy to use.\n",
    "-   On MacOS, you can access the command line interface by opening\n",
    "    Terminal.app.\n",
    "It also assumes that you have Python 3.6 or better installed. To test,\n",
    "run `python --version` from the command line, and note the version\n",
    "number that it prints. If this returns an error, you probably don’t have\n",
    "Python installed yet. One easy way to obtain a current version of Python\n",
    "is to install [Anaconda](https://docs.anaconda.com/anaconda/install), a\n",
    "free software package. Note that if you’re using Anaconda from within\n",
    "Windows Subsystem for Linux, you will want to install the Linux version,\n",
    "not the Windows version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6199408e",
   "metadata": {},
   "source": [
    "## Case resoration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932f005c",
   "metadata": {},
   "source": [
    "Nearly all speech and language technologies work by collecting\n",
    "statistics over huge collections of characters and/or words. While\n",
    "handful of words (like *the* or *she*) are very frequent, the vast\n",
    "majority of words (like *ficus* or *cephalic*) are quite rare. One of\n",
    "the major challenges in speech and language technology is making\n",
    "informed predictions about the linguistic behaviors of rare words.\n",
    "\n",
    "Many [writing systems](https://en.wikipedia.org/wiki/Writing_system),\n",
    "including those derived from the Greek, Latin, and Cyrillic alphabets,\n",
    "distinguish between upper- and lower-case words. Such writing systems\n",
    "are said to be *bicameral*, and those which do not make these\n",
    "distinctions are said to be *unicameral*. While casing can carry\n",
    "important semantic information (compare *bush* vs. *Bush*), this\n",
    "distinction also can introduce further sparsity to our data. Or as\n",
    "Church (1995) puts it, do we **really** need to keep totally separate\n",
    "statistics for *hurricane* or *Hurricane*, or can we merge them?\n",
    "\n",
    "In most cases, speech and language processing systems, including machine\n",
    "translation and speech recognition engines, choose to ignore casing\n",
    "distinctions; they\n",
    "[case-fold](https://docs.python.org/3/library/stdtypes.html#str.casefold)\n",
    "the data before training. While this is fine for many applications, it\n",
    "is often desirable to restore capitalization information afterwards,\n",
    "particularly if the text will be consumed by humans. Shugrina (2010)\n",
    "reports that users greatly prefer formatted transcripts over \"raw\"\n",
    "transcripts.\n",
    "\n",
    "Lita et al. (2003) introduce a task they call \"true-casing\". They use a\n",
    "simple machine learning model, a [*hidden Markov\n",
    "model*](https://en.wikipedia.org/wiki/Hidden_Markov_model), to predict\n",
    "the capitalization patterns of sentences word by word. They obtain good\n",
    "overall accuracy (well above 90%) when applying this method to English\n",
    "news text.\n",
    "\n",
    "In this exercise, we will develop a variant of the Lita et al. model,\n",
    "with some small improvements. The exercise is divided into six parts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3f9a05",
   "metadata": {},
   "source": [
    "## Structured classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8453cf42",
   "metadata": {},
   "source": [
    "True-casing is a *structured classification* problem, because the casing\n",
    "of one word depends on nearby words. While one could possibly choose to\n",
    "ignore this dependency (as would be necessary with a simple [Naïve\n",
    "Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) or\n",
    "[logistic regression](https://en.wikipedia.org/wiki/Logistic_regression)\n",
    "classifier), CRFSuite uses a first-order Markov model in which states\n",
    "represent casing tags, and the observations represent tokens. The best\n",
    "sequence of tags for a given sentence are computed using the [*Viterbi\n",
    "algorithm*](https://en.wikipedia.org/wiki/Viterbi_algorithm), which\n",
    "finds the best path by merging paths that share prefixes (e.g. the\n",
    "sequences \"NNN\" and \"NNV\" share the prefix \"NN\"). This merging\n",
    "calculates the probability of that prefix only once, as you can see in\n",
    "the figure below.\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/43279348/86036506-fcfbfa00-ba0b-11ea-819f-6a9f2bf86576.jpg\">\n",
    "\n",
    "Caching the intermediate results of these prefix paths to speed up\n",
    "calculations is an example of [*dynamic\n",
    "programming*](https://en.wikipedia.org/wiki/Dynamic_programming).\n",
    "Without this trick, it would take far too long to score every possible\n",
    "path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec00d33",
   "metadata": {},
   "source": [
    "## Part 1: the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7861f6ed",
   "metadata": {},
   "source": [
    "Read [Lita et al. 2003](https://www.aclweb.org/anthology/P03-1020/), the\n",
    "study which introduces the true-casing task. If you encounter unfamiliar\n",
    "jargon, look it up, or ask a colleague or instructor. Here are some\n",
    "questions about the reading intended to promote comprehension. Feel free\n",
    "to get creative, even if you don’t end up with the “right” answer.\n",
    "\n",
    "1.  The paper lists some examples of when truecasing might be useful\n",
    "    (automatic speech recognition, newspaper titles, etc.) Can you think\n",
    "    of any other cases where it might be helpful?\n",
    "2.  Not all languages’ writing systems distinguish between upper and\n",
    "    lower-case letters. Are any of the ideas here useful for these\n",
    "    languages? For example, the paper notes that \"\\[a\\]ccents can be\n",
    "    viewed as additional surface forms or alternate word casings.\"\n",
    "3.  What case is a number? For example, would you label *42* as\n",
    "    uppercase, lowercase, or something else?\n",
    "4.  In formula 1 (§2.2.1) label all of the variables ($P$,\n",
    "    $\\lambda$, and $w$). The authors do not explicitly state what\n",
    "    $\\lambda_{uniform} P_0$ is; what do you think it denotes?\n",
    "5.  In §2.2.2, the authors they describe which features go with each\n",
    "    node of the trellis. Which features are included in the trellis? Of\n",
    "    these features, which would you predict to be most useful for\n",
    "    predicting case? Can you think of any additional features which\n",
    "    might be useful to include?\n",
    "6.  The authors write that \"\\[t\\]he trellis can be viewed as a Hidden\n",
    "    Markov Model (HMM) computing the state sequence which best explains\n",
    "    the observations.\" What are the states of that HMM? What are the\n",
    "    observations? What are the transition probabilities?\n",
    "7.  The researchers use a trigram model to predict case. How might the\n",
    "    results have been different if they used a bigram or four-gram\n",
    "    model?\n",
    "8.  §2.3 discusses two possible approaches for dealing with unknown\n",
    "    words. What are they? What are the advantages and disadvantages of\n",
    "    each one?\n",
    "9.  For mixed-case tokens, there is no clear rule on which letters in\n",
    "    the word are capitalized. Consider *iPhone*, *LaTeX*, *JavaScript*,\n",
    "    and *McDonald’s*. What are some possible approaches to restoring the\n",
    "    case of mixed-case tokens?\n",
    "10. What data is this model trained on, and what are the benefits and\n",
    "    disadvantages of these datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1574106a",
   "metadata": {},
   "source": [
    "## Part 2: data and software"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45a1dc5",
   "metadata": {},
   "source": [
    "### What to do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed43d85c",
   "metadata": {},
   "source": [
    "1.  Obtain English data. Some tokenized English data from the Wall\n",
    "    St. Journal (1989-1990) portion of the Penn Treebank is available\n",
    "    [here](http://wellformedness.com/courses/wintercamp/data/wsj/).\n",
    "    These files cannot be distributed beyond our research group, so\n",
    "    ask Kyle for the password. Alternatively, one can download a year’s\n",
    "    worth of English data from the WMT News Crawl (2007) by executing\n",
    "    the following from the command line.\n",
    "\n",
    "        curl -C - http://data.statmt.org/news-crawl/en/news.2007.en.shuffled.deduped.gz -o \"news.2007.gz\"\n",
    "    \n",
    "    (Note that you can replace the `2007` above with any year from 2007 to\n",
    "    2019.) If you use the News Crawl data, you will need to tokenize it and\n",
    "    split into training, development, and testing sets to match the format\n",
    "    of the Wall St. Journal data. Therefore, write a Python script that\n",
    "    tokenizes the data (e.g., using\n",
    "    [`nltk.word_tokenize`](https://www.nltk.org/_modules/nltk/tokenize/punkt.html#PunktLanguageVars.word_tokenize)),\n",
    "    randomly splits the data into training (80%), development (10%), and\n",
    "    testing (10%), and writes the data to separate files (`train.tok`,\n",
    "    `dev.tok`, and `test.tok`).\n",
    "2.  Install the [CRFSuite](http://www.chokkan.org/software/crfsuite/)\n",
    "    tagger.\n",
    "    -   On Mac OS X, the easiest way to install CRFSuite is via\n",
    "        [Homebrew](https://brew.sh/).\n",
    "    -   Install Homebrew, if you have not already, by executing the\n",
    "        following command in Terminal.app, and following the on-screen\n",
    "        instructions.\n",
    "        \n",
    "            /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)\"\n",
    "    -   Then, to install CRFSuite, execute the following commands.\n",
    "    \n",
    "            brew tap brewsci/science\n",
    "            brew install crfsuite\n",
    "    -   On Linux and the Windows Subsystem for Linux, download and install\n",
    "        the program by executing the following commands in your system’s\n",
    "        terminal application.\n",
    "    \n",
    "            curl -LO https://github.com/downloads/chokkan/crfsuite/crfsuite-0.12-x86_64.tar.gz\n",
    "            tar -xzf crfsuite-0.12-x86_64.tar.gz\n",
    "            sudo mv crfsuite-0.12/bin/crfsuite /usr/local/bin\n",
    "\n",
    "    These three commands download, decompress, and install the program\n",
    "    into your path. Note that the last step may prompt you for your user\n",
    "    password."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d054838",
   "metadata": {},
   "source": [
    "## Part 3: training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15598aa",
   "metadata": {},
   "source": [
    "In this step you will train the case-restoration model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e3ae62",
   "metadata": {},
   "source": [
    "### Reading `case.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2bf007",
   "metadata": {},
   "source": [
    "One step during training requires us to tag tokens by their case. For\n",
    "instance, the sequence `He ate a prune wafer.` would be tagged as\n",
    "\n",
    "    He  TITLE    \n",
    "    ate LOWER \n",
    "    a   LOWER \n",
    "    prune   LOWER \n",
    "    wafer   LOWER\n",
    "    .   DC \n",
    "\n",
    "The `TITLE` tag refers to title-case, where only the first character is\n",
    "capitalized, and `DC` (\"don’t care\") is used for punctuation and digit\n",
    "tokens, which are neither upper- nor lower-case. Special treatment is\n",
    "required for mixed-casetokens like `LaTeX`. At the token, these are\n",
    "labeled `MIXED`, but additional data is needed to keep track of the\n",
    "character-level patterns. For instance, we might use the following tags:\n",
    "\n",
    "    L   UPPER\n",
    "    a   LOWER\n",
    "    T   UPPER  \n",
    "    e   LOWER\n",
    "    X   UPPER\n",
    "\n",
    "To tag tokens and characters, you will use the functions of a provided\n",
    "Python module, [`case.py`](src/case.py). If you’d like to go through the\n",
    "exercises below in a Jupyter notebook or code editor, ensure that\n",
    "`case.py` is in your working directory (i.e., the same directory as your\n",
    "Jupyter notebook), and then execute `import case` before beginning. We\n",
    "will proceed to go function by function.\n",
    "\n",
    "1. `def get_cc(nunichar: str) -> CharCase: ...`\n",
    "    1. What is the argument to `get_cc`? What is the argument’s type? What does `get_cc` return?\n",
    "        - Unicode character of type `str`. Returns `nunichar`'s character case\n",
    "    2. What do you obtain when you pass the following strings as arguments to this function: `\"L\"`, `\"a\"`, `\",\"`?\n",
    "        - UPPER, LOWER, DC\n",
    "    3. Which kinds of strings return the object `<CharCase.DC>`?\n",
    "        - Punctuation and digit tokens\n",
    "    4. Read the documentation for [`unicodedata`](https://docs.python.org/3/library/unicodedata.html), one of the libraries used to implement this function. Why does the argument have to be a single Unicode  character?\n",
    "        - As stated, the module provides access to the Unicode _Character_ Database therefore it process _characters_\n",
    "2. `def get_tc(nunistr: str) -> Tuple[TokenCase, Pattern]: ...`\n",
    "    1. What is the argument to `get_tc`? What is the argument’s type? What does `get_tc` return?\n",
    "        - \"A unicode string whose casing is to be computed\" of stype `str`.\n",
    "        The function returns the TokenCase of the input str and the pattern of the string if TokenCase is MIXED\n",
    "\n",
    "    2. What do you obtain when you pass the following strings as arguments to this function: `\"Mary\"`, `\"milk\"`, `\"LOL\"', and `\"LaTeX`?\n",
    "        - (TITLE, None), (LOWER, None), (UPPER, None), (MIXED, [UPPER, LOWER, UPPER, LOWER, UPPER])\n",
    "    3. What are the types of the first and second objects in the returned tuples?\n",
    "        - TokenCase and (Pattern | None)\n",
    "    4. Which of the strings above returns a list as the second object in the tuple? What do the elements in that list tell us about the string?\n",
    "        - \"LaTeX\". The elements in that list tell us the casing of the individual characters in that string.\n",
    "3. `def apply_cc(nunichar: str, cc: CharCase) -> str: ...`\n",
    "    1. What are the arguments to `apply_cc`? What is their types? What does `apply_cc` return?\n",
    "        - nunichar, str, the character passed in the desired casing\n",
    "    2. Apply `CharCase.UPPER` to the following strings: `\"L\"`, `\"A\"`. What do you obtain?\n",
    "        - You obtain what you passed as both chars are already UPPER\n",
    "    3. Repeat the previous step but with `CharCase.LOWER`. What do you obtain?\n",
    "        - You obtain \"l\", \"a\"\n",
    "    4. Read the tests in [`case_test.py`](src/case_test.py) to see how they use `apply_cc`.\n",
    "4.  `def apply_tc(nunistr: str, tc: TokenCase, pattern: Pattern = None) -> str: ...`\n",
    "    1. What are the arguments to `apply_tc`? What are the arguments’ types?What does `apply_tc` return?\n",
    "        -  nunistr, tc and pattern | str, TokenCase and Pattern | returns str passed in desired TokenCase and Pattern\n",
    "    2. Apply `TokenCase.LOWER` to the following strings: `\"Mary\"`, `\"milk\"`, `\"LOL\"', and `\"LaTeX\"`. What do you obtain?\n",
    "        - \"mary\", \"milk\", \"lol\", \"latex\"\n",
    "    3. Repeat the previous step but with `TokenCase.TITLE` and `TokenCase.UPPER`. What do you obtain?\n",
    "        - \"Mary\", \"Milk\", \"Lol\", \"Latex\"\n",
    "        - \"MARY\", \"MILK\", \"LOL\", \"LATEX\"\n",
    "    4. Read the tests in [`case_test.py`](src/case_test.py) to see how they use `apply_tc`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ecbf5a",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872244b1",
   "metadata": {},
   "source": [
    "During both training and prediction, the model must be provided with\n",
    "features used to determine the casing pattern for each token. Minimally,\n",
    "these features should include:\n",
    "\n",
    "1.  The target token\n",
    "2.  The token to the left (or `__BOS__` if the token is\n",
    "    sentence-initial)\n",
    "3.  The token to the right (or `__EOS__` if the token is sentence-final)\n",
    "4.  The conjunction of #2 and #3\n",
    "\n",
    "Optionally, one may also provide features such as:\n",
    "\n",
    "1.  The token two to the left (or `__BOS__`)\n",
    "2.  The token two to the right (or `__EOS__`)\n",
    "3.  Prefixes and/or suffixes of the target token.\n",
    "\n",
    "CRFSuite requires feature files for both training and prediction. Each\n",
    "line consists of a single token’s features, separated by a tab (`\\t`)\n",
    "character, with a blank line between each sentence. Feature files for\n",
    "training should also include the tag itself as the first column in the\n",
    "feature. Thus, for the sentence\n",
    "\n",
    "    Nelson Holdings International Ltd. dropped the most on a percentage basis , to 1,000 shares from 255,923 .\n",
    "\n",
    "the training feature file might look a bit like:\n",
    "\n",
    "    TITLE   t[0]=nelson     __BOS__ suf1=n  suf2=on suf3=son\n",
    "    TITLE   t[0]=holdings   t[-1]=nelson    t[+1]=international     t[-1]=nelson^t[+1]=international        suf1=s  suf2=gs            suf3=ngs\n",
    "    TITLE   t[0]=international      t[-1]=holdings  t[+1]=ltd.      t[-1]=holdings^t[+1]=ltd.       t[-2]=nelson    t[+2]=dropped      suf1=l   suf2=al suf3=nal\n",
    "    TITLE   t[0]=ltd.       t[-1]=international     t[+1]=dropped   t[-1]=international^t[+1]=dropped       t[-2]=holdings t[+2]=the   suf1=.   suf2=d. suf3=td.\n",
    "    LOWER   t[0]=dropped    t[-1]=ltd.      t[+1]=the       t[-1]=ltd.^t[+1]=the    t[-2]=international     t[+2]=most     suf1=d      suf2=ed  suf3=ped\n",
    "    LOWER   t[0]=the        t[-1]=dropped   t[+1]=most      t[-1]=dropped^t[+1]=most        t[-2]=ltd.      t[+2]=on       suf1=e      suf2=he\n",
    "    LOWER   t[0]=most       t[-1]=the       t[+1]=on        t[-1]=the^t[+1]=on      t[-2]=dropped   t[+2]=a suf1=t  suf2=st            suf3=ost\n",
    "    LOWER   t[0]=on t[-1]=most      t[+1]=a t[-1]=most^t[+1]=a      t[-2]=the       t[+2]=percentage        suf1=n\n",
    "    LOWER   t[0]=a  t[-1]=on        t[+1]=percentage        t[-1]=on^t[+1]=percentage       t[-2]=most      t[+2]=basis\n",
    "    LOWER   t[0]=percentage t[-1]=a t[+1]=basis     t[-1]=a^t[+1]=basis     t[-2]=on        t[+2]=, suf1=e  suf2=ge suf3=age\n",
    "    LOWER   t[0]=basis      t[-1]=percentage        t[+1]=, t[-1]=percentage^t[+1]=,        t[-2]=a t[+2]=to        suf1=s suf2=is     suf3=sis\n",
    "    DC      t[0]=,  t[-1]=basis     t[+1]=to        t[-1]=basis^t[+1]=to    t[-2]=percentage        t[+2]=1,000\n",
    "    LOWER   t[0]=to t[-1]=, t[+1]=1,000     t[-1]=,^t[+1]=1,000     t[-2]=basis     t[+2]=shares    suf1=o\n",
    "    DC      t[0]=1,000      t[-1]=to        t[+1]=shares    t[-1]=to^t[+1]=shares   t[-2]=, t[+2]=from      suf1=0  suf2=00            suf3=000\n",
    "    LOWER   t[0]=shares     t[-1]=1,000     t[+1]=from      t[-1]=1,000^t[+1]=from  t[-2]=to        t[+2]=255,923   suf1=s suf2=es     suf3=res\n",
    "    LOWER   t[0]=from       t[-1]=shares    t[+1]=255,923   t[-1]=shares^t[+1]=255,923      t[-2]=1,000     t[+2]=. suf1=m suf2=om     suf3=rom\n",
    "    DC      t[0]=255,923    t[-1]=from      t[+1]=. t[-1]=from^t[+1]=.      suf1=3  suf2=23 suf3=923\n",
    "    DC      t[0]=.  __EOS__\n",
    "\n",
    "and for prediction, one would simply omit the first column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83c0dcc",
   "metadata": {},
   "source": [
    "#### What to do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd94b71",
   "metadata": {},
   "source": [
    "Write a function which, given a sentence, extracts a list of list of\n",
    "features for that sentence. It might have the following signature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae73568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(tokens: List[str]) -> List[List[str]]:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9395f28",
   "metadata": {},
   "source": [
    "Then, write a script which uses this function to generate a feature\n",
    "file. To make this work, you will want to first split each sentence into\n",
    "tokens, call `extract` to obtain the features, then, for each token,\n",
    "print the tag and the features, separated by tab. Remember to also print\n",
    "a blank line between each sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b820b9",
   "metadata": {},
   "source": [
    "#### Hints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f183a4fa",
   "metadata": {},
   "source": [
    "-   If you get stuck, you may want to \"peek\" at\n",
    "    [`features.py`](src/features.py), which contains a draft of the\n",
    "    feature extractor function itself.\n",
    "-   CRFSuite follows a convention whereby a `:` in a feature is\n",
    "    interpreted as a feature weight. Therefore we suggest that you\n",
    "    replace `:` in any token with another character such as `_`. If this\n",
    "    is done consistently for all feature files, this is extremely\n",
    "    unlikely to result in a loss of information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b27620",
   "metadata": {},
   "source": [
    "### The mixed-case dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3670990",
   "metadata": {},
   "source": [
    "Mixed-case tokens like `McDonald's` and `LaTeX` all have different\n",
    "mixed-case patterns. It is not enough to know that they are mixed: one\n",
    "also has to know which mixed-case pattern they follow. One can make a\n",
    "simplifying assumption that few if any mixed-case tokens vary in which\n",
    "mixed-case pattern they follow (or that such variation is mostly\n",
    "erroneous), and therefore one simply needs to store a table with the\n",
    "most-common pattern for each mixed-case token, which is used to produce\n",
    "the proper form of a token tagged as mixed-case. This table is computed\n",
    "in two steps.\n",
    "\n",
    "1.  First, count the frequency of each mixed-case pattern for each\n",
    "    token.\n",
    "2.  Then, select only the most frequent mixed-case pattern for each\n",
    "    token.\n",
    "\n",
    "For step #1, we use a dictionary whose keys are case-folded strings\n",
    "whose values are\n",
    "[`collections.Counter`](https://docs.python.org/3/library/collections.html#collections.Counter)\n",
    "objects containing the mixed-case form. For instance, this might\n",
    "resemble the following:\n",
    "\n",
    "```python\n",
    "{'iphone' : Counter{'iPhone': 11, 'IPhone': 5, 'iphone': 3}, ...}\n",
    "```\n",
    "\n",
    "Then, for step #2, we create a simpler dictionary which contains just\n",
    "the most frequent mixed-case pattern as the value, like the following:\n",
    "\n",
    "```python\n",
    "{'iphone': 'iPhone', ...}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b729f52",
   "metadata": {},
   "source": [
    "#### What to do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa5ee61",
   "metadata": {},
   "source": [
    "Write a script which reads a tokenized file, constructs the second\n",
    "dictionary, and then writes it to disk as a [JSON\n",
    "file](https://docs.python.org/3/library/json.html) using\n",
    "[`json.dump`](https://docs.python.org/3/library/json.html). You may wish\n",
    "to combine this script with the code you wrote in the previous step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2cbfe2",
   "metadata": {},
   "source": [
    "#### Hints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59abdff8",
   "metadata": {},
   "source": [
    "-   If you get stuck, you may want to carefully read the documentation\n",
    "    for\n",
    "    [`collections.Counter`](https://docs.python.org/3/library/collections.html#collections.Counter)\n",
    "    and\n",
    "    [`collections.defaultdict`](https://docs.python.org/3/library/collections.html#collections.defaultdict),\n",
    "    particularly the [provided\n",
    "    examples](https://docs.python.org/3/library/collections.html#defaultdict-examples).\n",
    "-   You will need to create an empty dictionary before looping over the\n",
    "    data. This may look something like the following.\n",
    "    ```python\n",
    "    mcdict = collections.defaultdict(collections.Counter)\n",
    "    ```\n",
    "    Then, inside the loop(s), you can add a count to the dictionary as in the following snippet.\n",
    "    ```python\n",
    "    mcdict[token.casefold()][token] += 1\n",
    "    ```\n",
    "-   If you are working with very large data sets, you may want to\n",
    "    slightly modify step #2 to exclude tokens that occur in mixed-case\n",
    "    very infrequently (e.g., less than twice) on the hypothesis that\n",
    "    such tokens are typographical errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f180e60",
   "metadata": {},
   "source": [
    "### CRF training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9877501",
   "metadata": {},
   "source": [
    "You are now almost ready to train the model itself. To do this you will\n",
    "need\n",
    "\n",
    "-   a feature file (including tags) `train.features` for training data\n",
    "    `train.tok`, and\n",
    "-   a feature file (including tags) `dev.features` for development data\n",
    "    `dev.tok`,\n",
    "\n",
    "which can be created using the `extract` function described above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d58ccf1",
   "metadata": {},
   "source": [
    "#### What to do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187d2540",
   "metadata": {},
   "source": [
    "At the command line, issue the following command to train the CRF model:\n",
    "\n",
    "    crfsuite learn \\\n",
    "        -p feature.possible_states=1 \\\n",
    "        -p feature.possible_transitions=1 \\\n",
    "        -m model \\\n",
    "        -e2 train.features dev.features\n",
    "\n",
    "This will train the model and write the result (in a non-human-readable\n",
    "format) to the file `model`; training may take up to several minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c34ffa0",
   "metadata": {},
   "source": [
    "## Part 4: prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b16fa0",
   "metadata": {},
   "source": [
    "To predict, or restore case to the tokens in `test.tok` using the model\n",
    "you trained in **Part 3**, complete the following steps.\n",
    "\n",
    "1. Extract features from `test.tok` and write them to `test.features`. This file should **not** include case tags.\n",
    "2. At the command line, issue the following command to apply the model you trained in **Part 3**:\n",
    "\n",
    "       crfsuite tag -m model test.features > test.predictions\n",
    "\n",
    "3. Using the predicted tags in `test.predictions`, the mixed-cased dictionary, and the family of `apply_*` functions in `case.py`, apply casing to case-folded tokens in `test.tok`, and then write these restored-case tokens to a file formatted similarly to `test.tok`, with one sentence per line and space between each token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5af384",
   "metadata": {},
   "source": [
    "## Part 5: evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08eb7a6",
   "metadata": {},
   "source": [
    "So, how good is your true-caser? There are many ways we can imagine\n",
    "measuring this. One could ask humans to rate the quality of the output\n",
    "casing, and one might even want to take into account how often two\n",
    "humans agree about whether a word should or should not be capitalized.\n",
    "However a simpler evaluation (and one which does not require humans \"in\n",
    "the loop\" is to compute token-level accuracy. Accuracy can be thoguht\n",
    "of as the probability that a randomly selected token will receive the\n",
    "correct casing.\n",
    "\n",
    "While it is possible to compute accuracy directly on the tags produced\n",
    "by `crfsuite`, this has the risk of slightly underestimating the actual\n",
    "accuracy. For instance, if the system tags a punctuation character as\n",
    "`UPPER`, this seems wrong, but it is harmless; punctuation tokens are\n",
    "inherently case-less and it doesn’t matter what kind of tag it receives.\n",
    "When multiple predictions all give the right \"downstream\" answer, the\n",
    "model is said to exhibit *spurious ambiguity*; a good evaluation method\n",
    "should not penalize spurious ambiguity. In this case, one can avoid\n",
    "spurious ambiguity by evaluating not on the tags but on the tokenized\n",
    "data, after it has been converted back to that format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a588f41",
   "metadata": {},
   "source": [
    "### What to do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0e4808",
   "metadata": {},
   "source": [
    "Write a script called `evaluate.py`. It should take two command-line\n",
    "arguments: the path to the original \"gold\" tokenized and cased data, and\n",
    "the path to the predicted data from the previous step. It should first\n",
    "initialize two counters, one for the number of correctly cased tokens,\n",
    "and one for the total number of tokens. Then, iterating over the two\n",
    "files, count the number of correctly cased tokens, the number of overall\n",
    "tokens. To compute accuracy, it should divide the former by the latter,\n",
    "round to 3-6 digits, and print the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b31e7f",
   "metadata": {},
   "source": [
    "### Hint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e9b9a9-d407-46ec-b4b9-62f6b0700436",
   "metadata": {},
   "source": [
    "Your evaluation script should **not** read both files all at once, which will not work for very large files. Rather it should process the data line by line. For instance, if the gold data file handle is `gold` and the predicted data file handle is `pred`, part of your script might resemble the following.\n",
    "\n",
    "```python\n",
    "for gold_line, pred_line in zip(gold, pred):\n",
    "    gold_tokens = gold_line.split()\n",
    "    pred_tokens = pred_line.split()\n",
    "    assert len(gold_tokens) == len(pred_tokens), \"Mismatched lengths\"\n",
    "    for gold_token, pred_token in zip(gold_tokens, pred_tokens):\n",
    "       ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b105599",
   "metadata": {},
   "source": [
    "## Part 6: style transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3347c4",
   "metadata": {},
   "source": [
    "*Style transfer* refers to the use of machine learning to apply the\n",
    "\"style\" of a *reference* medium (e.g., texts, images, or videos) to to\n",
    "some other resource of the same type. For instance, in computer vision,\n",
    "researchers have used this technique to [transfer the styles of Picasso,\n",
    "Van Gogh, and Monet onto a painting of da\n",
    "Vinci](https://genekogan.com/works/style-transfer/). In this section,\n",
    "you will use the true-casing model to transfer the \"casing style\" from\n",
    "some novel corpus to the data you used above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0869b4e8",
   "metadata": {},
   "source": [
    "### What to do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f646c97c",
   "metadata": {},
   "source": [
    "1.  Obtain a reference corpus. A pre-tokenized corpus of tweets by\n",
    "    [@realDonaldTrump](https://twitter.com/realDonaldTrump) is available\n",
    "    [here](http://wellformedness.com/courses/wintercamp/data/trump/).\n",
    "    Alternatively, one can obtain data from some other source, such as\n",
    "    the social media accounts of other famous personages (e.g.,\n",
    "    [@dril](https://twitter.com/dril),\n",
    "    [@FINALLEVEL](https://twitter.com/finallevel)), and then tokenize\n",
    "    the data as described in Part 2.\n",
    "2.  Using the data from the previous step, training a casing model as in\n",
    "    Part 3.\n",
    "3.  Using the model from the previous step, apply this model to the test\n",
    "    data from Part 2 as in Part 4.\n",
    "4.  Compare, manually or automatically, the predictions of the\n",
    "    style-transfer model to those of the in-domain model. How do they\n",
    "    differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e438d5",
   "metadata": {},
   "source": [
    "## Postscript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba57b68",
   "metadata": {},
   "source": [
    "### Further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716bfe15",
   "metadata": {},
   "source": [
    "Other interesting work on case-restoration includes Chelba & Acero 2006\n",
    "and Wang et al. 2006."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a2d866",
   "metadata": {},
   "source": [
    "### Stretch goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6ece20",
   "metadata": {},
   "source": [
    "1.  Above, the act of training the model and casing data required\n",
    "    several commands. It may be more convenient to combine the steps\n",
    "    into two programs,\n",
    "    -   a trainer which converting tokenized data to two-column format and\n",
    "        invokes `crfsuite learn` on it, and\n",
    "    -   a predictor which converts tokenized data to one-column format,\n",
    "        invokes `crfsuite tag`, and then converts it back to tokenized\n",
    "        format.\n",
    "    The obvious way to do this is to write two Python scripts which, as\n",
    "    part of their process, call shell commands using the built-in\n",
    "    [`subprocess`](https://docs.python.org/3/library/subprocess.html)\n",
    "    module. In particular, use\n",
    "    [`subprocess.check_call`](https://docs.python.org/3/library/subprocess.html#subprocess.check_call),\n",
    "    which executes a shell command and raises an error if it fails, or\n",
    "    [`subprocess.popen`](https://docs.python.org/3/library/subprocess.html#popen-constructor),\n",
    "    which has a similar syntax but also captures the output of the\n",
    "    command. Since both these scripts must generate temporary files\n",
    "    (i.e., the data in one- or two-column format) you may want to use\n",
    "    the built-in\n",
    "    [`tempfile`](https://docs.python.org/3/library/tempfile.html)\n",
    "    module, and in particular\n",
    "    [`tempfile.NamedTemporaryFile`](https://docs.python.org/3/library/tempfile.html#tempfile.NamedTemporaryFile)\n",
    "    or\n",
    "    [`tempfile.mkstemp`](https://docs.python.org/3/library/tempfile.html#tempfile.mkstemp).\n",
    "    Since these scripts will need to take various arguments (paths to\n",
    "    input and output files, as well as the model order hyperparameters),\n",
    "    use the built-in\n",
    "    [`argparse`](https://docs.python.org/3/library/argparse.html) module\n",
    "    to parse command-line flags.\n",
    "2.  Convert your implementation to a [Python\n",
    "    package](https://packaging.python.org/) which can be installed using\n",
    "    `pip`. If you are also doing stretch goal #1, you may want to make\n",
    "    the trainer and the predictor separate [console\n",
    "    scripts](https://python-packaging.readthedocs.io/en/latest/command-line-scripts.html#the-console-scripts-entry-point).\n",
    "    Make sure to fail gracefully if the user doesn’t already have\n",
    "    `crfsuite` installed. Alternatively, you can build your package\n",
    "    around the\n",
    "    [`python-crfsuite`](https://github.com/scrapinghub/python-crfsuite)\n",
    "    package, available from `pip`, which exposes the CRFSuite internals\n",
    "    directly to Python, without the need to call the `crfsuite` binary.\n",
    "3.  Train and evaluate a model on a language other than English (though\n",
    "    make sure the writing system makes case distinctions—not all do);\n",
    "    even French and German both have rather different casing rules.\n",
    "4.  Train, evaluate, and distribute a **gigantic** case restoration\n",
    "    model using a megacorpus such as\n",
    "    -   [Gigaword](https://catalog.ldc.upenn.edu/LDC2011T07),\n",
    "    -   the [Billion Word\n",
    "        Benchmark](https://github.com/ciprian-chelba/1-billion-word-language-modeling-benchmark),\n",
    "        or\n",
    "    -   multiple years of the [WMT news\n",
    "        crawl](https://gist.github.com/kylebgorman/5109b09fbfc3a2c1dbbdd405326c1130)\n",
    "        data.\n",
    "    To prevent the number of features from swamping your training, you\n",
    "    may wish to use\n",
    "    [`-p feature.minfreq=`](http://www.chokkan.org/software/crfsuite/manual.html#idp8853519024)\n",
    "    when training. For instance, if you call\n",
    "    `crfsuite learn -p feature.minfreq=10 ...`, then any feature\n",
    "    occurring less than 10 times in the training data will be ignored.\n",
    "3.  Above we provided a relatively simple feature extraction function.\n",
    "    Would different features do better? Add, remove, or combine\n",
    "    features, retrain your model, and compare the results the provided\n",
    "    feature function.\n",
    "4.  Alternatively, you can try a different type of model altogether\n",
    "    using the\n",
    "    [`perceptronix`](https://github.com/kylebgorman/perceptronix/)\n",
    "    library, which provides a fast C++-based backend for training linear\n",
    "    sequence models with the perceptron learning algorithm. Install\n",
    "    Perceptronix, then using `case.py` and the\n",
    "    `perceptronix.SparseDenseMultinomialSequentialModel` class, build a\n",
    "    discriminative case restoration engine and compare the results to\n",
    "    the CRF model.\n",
    "4.  Section 2.3 of Lita et al. proposes an alternative method for\n",
    "    handling rare words. Re-read that section and implement their\n",
    "    proposal, then compare it to your earlier implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6fc6b4",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab413eff",
   "metadata": {},
   "source": [
    "Chelba, C. and Acero, A. 2006. Adaptation of maximum entropy\n",
    "capitalizer: little data can help a lot. *Computer Speech and Language*\n",
    "20(4): 382-39.\n",
    "\n",
    "Church, K. W. 1995. One term or two? In *Proceedings of the 18th Annual\n",
    "International ACM SIGIR conference on Research and Development in\n",
    "Information Retrieval*, pages 310-318.\n",
    "\n",
    "Lafferty, J., McCallum, A., and Pereira, F. 2001. Conditional random\n",
    "fields: probabilistic models for segmenting and labeling sequence data.\n",
    "In *Proceedings of the 18th International Conference on Machine\n",
    "Learning*, pages 282-289.\n",
    "\n",
    "Lita, L. V., Ittycheriah, A., Roukos, S. and Kambhatla, N. 2003.\n",
    "tRuEcasIng. In *Prooceedings of the 41st Annual Meeting of the\n",
    "Association for Computational Linguistics*, pages 152-159.\n",
    "\n",
    "Shugrina, M. 2010. Formatting time-aligned ASR transcripts for\n",
    "readability. In *Human Language Technologies: The 2010 Annual Conference\n",
    "of the North American Chapter of the Association for Computational\n",
    "Linguistics*, pages 198-206.\n",
    "\n",
    "Wang, W., Knight, K., and Marcu, D. 206. Capitalizing machine\n",
    "translation. In *Proceedings of the Human Language Technology Conference\n",
    "of the NAACL, Main Conference*, pages 1-8."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}